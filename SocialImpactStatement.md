
## Social Impact Statement

According to the FAT/ML organization, it is important to outline a Social Impact Statement for Algorithms that have an impact on the public. 

This Social Impact Statement should be reassessed at least three times during the design and development process:

- design stage,
- pre-launch,
- and post-launch.

Also, after the app is launched, the statement should be made public as a form of transparency, so that the public has expectations for the social impact of the system.

Below are areas addressed according to the FAT/ML Accountable Algorithms framework.

## Responsibility

The topic analysis algorithm will only serve as an assistant to the editors and should not be used to generate content for the users. The editors and development team members have the final say on what content gets published and should take full responsibility for the content displayed to readers.

A process for identifying and addressing reporting errors should be created and frequently checked. An individual should be delegated the responsibility for responding to reported errors. The contact information for this person should also be available to users of the platform so that they can report mistakes if they find them.

## Explainability

On the app, users should be able to access a page about the editorial methodologies and algorithms used. A paragraph and diagram should explain the function and purpose of each algorithm in understandable language and diagrams. This page should also disclose the full list of journalistic sources that are used for content curation. It should also include short biographies of the current editors with their methodology and work experience. Including this information creates transparency in the editor's news aggregation process and builds trust with the readers of the app. 

## Accuracy

The accuracy of the algorithm should be quantitatively measured. The algorithm must return a reasonable accuracy before being deemed usable by the editors.

The developers and editors should also keep an eye out for any garbled data from the web scraping platform. By looking at a random sample of the daily dataset, they should check to see if the collected articles are not readable. If illegible articles are found, they should be removed from the dataset and the web scraper should be improved.  

## Auditing

The algorithm can reside on shared Co-lab documents or on a private GitHub repo to allow for 3rd party auditing. An API can be created to allow third parties to query the algorithmic system and assess its response.

The data for the algorithm is acquired from a wide variety of trusted news sources. By diversifying the source of the data, it makes it more difficult for the data to become corrupted to the detriment to the algorithm. A dataset of archived news stories can be made available for others to use to test the algorithm.


## Fairness

A limitation of using only major news sources for article analysis is there will be less representation of underrepresented writers. This is due to current biases in the news industry and social capital barriers. Fortunately, there have been more diversity and inclusion efforts in the journalism industry, so this will improve over time. In the meantime, an editor can manually add in articles from journalists with a greater diversity of perspectives, especially when articles pertain to identity-based news or the event occurs in a town with a local news source. In order for groups of people to represent their own communities, the editors should be mindful of the journalist's, race, sex, gender identity, ability status, socio-economic status, education level, religion, and/or country of origin when it is relevant to the topic of the article.
