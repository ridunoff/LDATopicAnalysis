
## Social Impact Statement

According to the FAT/ML organization, it is important to outline a Social Impact Statement for Algorithms that have impact on the public. 

This Social Impact Statement should be reassessed at least three times during the design and development process:

- design stage,
- pre-launch,
- and post-launch.

Also, after the app is launched, the statement should be made public as a form of transparency, so that the public has expectations for the social impact of the system.

Below are the areas we have addressed according to the FAT/ML Accountable Algorithms framework.

## Responsibility

The topic analysis algorithm will only serve as an assistant to the editors and should not be used to generate content for the users. The editors and development team members have the final say on what content gets published and should take full responsibility for the content displayed to readers.

A process for identifying and addressing reporting errors should be created and frequently checked. An individual should be delegated the responsibility for responding to reported errors. The contact information for this person should also be available to users of the platform so that they are able to report mistakes if they find them.

## Explainability

On the app, users should be able to access a page about the editorial methodologies and algorithms used. A paragraph and diagram should explain the function and purpose of each algorithm in understandable language and diagrams. This page should also disclose the full list of journalistic sources that are used for content curation. It should also include short biographies of the current editors with their methodology and work experience.

## Accuracy

The accuracy of the algorithm should be quantitatively measured. The algorithm must return a reasonable accuracy before being deemed usable by the editors.

Also, the editors have final say onto if the results of the topic sorting algorithm get used or not. 

The developers and editors should also keep an eye out for any garbled data from the web scraping platform by looking at a random sample. If the collected articles are not readable, they should not be used in the dataset to improve the accuracy as well.  

## Auditing

The algorithm can reside on shared co-lab documents or on a private github repo to allow for 3rd party auditing.

Since our data comes from a wide variety of trusted sources, it is unlikely that an individual has the power to influence and manipulate a large portion of the media to detriment the algorithm.

If necessary, further down the line, an API could be created to allow third parties to query the algorithmic system and assess its response. Also, a dataset of older news could be made available for others to use to test the algorithm.

## Fairness

One limitation of only using major news sources for article analysis there will be less representation of underrepresented writers. This is due to current biases in the news industry and social capital barriers. Fortunately, there have been more diversity and inclusion efforts in the journalism industry, so this will improve over time. For the meantime, an editor can manually add in articles from journalists with a greater diversity of perspectives, especially when articles pertain identity-based new or the event occurs in a town with a local news source. In order for groups of people to represent their own communities, the  editors should be mindful of the journalist's, race, sex, gender identity, ability status, socio-economic status, education level, religion, and/or country of origin when it is relevant to the topic of the article.
